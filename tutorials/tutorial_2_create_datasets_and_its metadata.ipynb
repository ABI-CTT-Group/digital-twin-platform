{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Create datasets and its metadata\n",
    "This tutorial shows how to create datasets and its metadata in SPARC Data Structure (SDS) format via SPARC-me API. In this tutorial, you will learn:\n",
    "* How to create a SDS template dataset or load an existing SDS dataset.\n",
    "* How to retrieve the metadata and its values from the dataset and save the dataset locally.\n",
    "* How to add or update metadata values.\n",
    "* How to add raw data to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an empty SDS dataset, or load an existing one if your dataset is based on SDS v2.0.0\n",
    "* Setup sparc-me and create an instance of the `Dataset` class\n",
    "\n",
    "    You need to install the SPARC-me package for this tutorial. Sparc-me is a python tool to explore, enhance, and expand SPARC datasets and their descriptions in accordance with FAIR principles.\n",
    "\n",
    "    `pip install sparc-me`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparc_me import Dataset, Schema, Validator\n",
    "\n",
    "dataset = Dataset()\n",
    "schema = Schema()\n",
    "validator = Validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an empty dataset from the SDS template\n",
    "\n",
    "    In this tutorial, it is highly recommended to use SDS template version 2.0.0. Remember to include the path of where you want your dataset to be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset path\n",
    "save_dir = \"SDStemplate/\"\n",
    "dataset.set_dataset_path(save_dir)\n",
    "dataset.load_from_template(version=\"2.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save the template dataset that you just loaded\n",
    "\n",
    "    Explore the SDS template dataset after it's saved locally. __Restart the kernel if you have problems saving the dataset.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save(save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load an existing SDS dataset (not recommended in this tutorial unless your dataset is based on SDS template version 2.0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_dataset_path is optional here. You can either set a new path or using the dataset's current path for loading the existing dataset\n",
    "# dataset.set_dataset_path(save_dir)\n",
    "# existing_dir = \"./your/dataset/path/here\"\n",
    "# dataset.load_dataset(dataset_path=existing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the metadata and its associated values. Clear the values prior to updating.\n",
    "This tutorial focus more on the dataset_description metadata file. For other metadata files, replace the category field with the name of the metadata file you want to use e.g., from `category=\"dataset_description\"` to `category=\"code_parameters\"`.\n",
    "\n",
    "* List all metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories:\n",
      "code_description\n",
      "code_parameters\n",
      "dataset_description\n",
      "manifest\n",
      "performances\n",
      "resources\n",
      "samples\n",
      "subjects\n",
      "submission\n"
     ]
    }
   ],
   "source": [
    "categories = dataset.list_categories(version=\"2.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* List the elements of a metadata file, e.g., dataset_description metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields:\n",
      "Metadata Version\n",
      "Type\n",
      "Basic information\n",
      "Title\n",
      "Subtitle\n",
      "Keywords\n",
      "Funding\n",
      "Acknowledgments\n",
      "Study information\n",
      "Study purpose\n",
      "Study data collection\n",
      "Study primary conclusion\n",
      "Study organ system\n",
      "Study approach\n",
      "Study technique\n",
      "Study collection title\n",
      "Contributor information\n",
      "Contributor name\n",
      "Contributor ORCiD\n",
      "Contributor affiliation\n",
      "Contributor role\n",
      "Related protocol, paper, dataset, etc.\n",
      "Identifier description\n",
      "Relation type\n",
      "Identifier\n",
      "Identifier type\n",
      "Participant information\n",
      "Number of subjects\n",
      "Number of samples\n"
     ]
    }
   ],
   "source": [
    "elements = dataset.list_elements(category=\"dataset_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get schema information of an element of a metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metadata Version': {'type': 'string', 'required': 'Y', 'description': 'SDS version number, e.g. 2.0.0. DO NOT CHANGE', 'example': '2.0.0'}, 'Type': {'type': 'string', 'required': 'Y', 'description': 'Each dataset consists of a single “type” of data, covered by the same ethics, same access control, same protocol, etc. There are two datasets types,  experimental and computation. Experimental is the default value. Make sure to change it to computation only if you are submitting a computational study. If the dataset is not” computational”, it should be set to “experimental”', 'example': 'experimental'}, 'Title': {'type': 'string', 'required': 'Y', 'description': 'Descriptive title for the data set. Equivalent to the title of a scientific paper.', 'example': 'My SPARC dataset'}, 'Subtitle': {'type': 'string', 'required': 'Y', 'description': 'Brief description of the study and the data set. Equivalent to the abstract of a scientific paper. Include the rationale for the approach, the types of data collected, the techniques used, formats and number of files and an approximate size.', 'example': 'A really cool dataset that I collected to answer some question.'}, 'Keywords': {'type': 'string', 'required': 'Y', 'description': 'A set of keywords to assist in search.', 'example': 'spinal cord, electrophysiology, RNA-seq, mouse'}, 'Funding': {'type': 'string', 'required': 'N', 'description': 'Funding sources', 'example': 'OT2OD025349'}, 'Acknowledgments': {'type': 'string', 'required': 'N', 'description': 'Acknowledgments beyond funding and contributors.', 'example': 'Thank you everyone!'}, 'Study purpose': {'type': 'string', 'required': 'Y', 'description': 'A description of the study purpose for the structured abstract.', 'example': 'This study was conducted to demonstrate data wranglers how to fill out dataset templates.'}, 'Study data collection': {'type': 'string', 'required': 'Y', 'description': 'A description of the study data collection process for this dataset.', 'example': 'Using an earlier version of this template we measured how much it confused data wranglers by counting the number of emails we had to exchange with them in order to fill it out.'}, 'Study primary conclusion': {'type': 'string', 'required': 'Y', 'description': 'A description of the primary conclusion drawn from the study for the structured abstract.', 'example': 'The primary conclusion of this study is that it is hard to make a good dataset template.'}, 'Study organ system': {'type': 'string', 'required': 'Y', 'description': 'The major organ systems related to this study.', 'example': 'spinal cord'}, 'Study approach': {'type': 'string', 'required': 'Y', 'description': 'The experimental approach or approaches taken in this study.', 'example': 'electrophysiology'}, 'Study technique': {'type': 'string', 'required': 'Y', 'description': 'The experimental techniques used in this study.', 'example': 'patch clamp'}, 'Study collection title': {'type': 'string', 'required': 'N', 'description': 'Title of the larger collection to which this dataset belongs.', 'example': 'My SPARC research study'}, 'Contributor name': {'type': 'string', 'required': 'Y', 'description': ' Name of any contributors to the dataset.  These individuals need not have been authors on any publications describing the data, but should be acknowledged for their role in producing and publishing the data set.  If more than one, add each contributor in a new column.', 'example': 'Last, First Middle'}, 'Contributor ORCiD': {'type': 'string', 'required': 'Y', 'description': 'ORCiD ID', 'example': 'https://orcid.org/0000-0002-5497-0243'}, 'Contributor affiliation': {'type': 'string', 'required': 'Y', 'description': 'Institutional affiliation for contributors', 'example': 'https://ror.org/0168r3w48'}, 'Contributor role': {'type': 'string', 'required': 'Y', 'description': 'Contributor role. At most one PrincipalInvestigator and at least one CorrespondingAuthor are required. These roles are provided by the Data Cite schema. Options are:\\nPrincipalInvestigator\\nCreator\\nCoInvestigator\\nCorrespondingAuthor\\nDataCollector\\nDataCurator\\nDataManager\\nDistributor\\nEditor\\nProducer\\nProjectLeader\\nProjectManager\\nProjectMember\\nRelatedPerson\\nResearcher\\nResearchGroup\\nSponsor\\nSupervisor\\nWorkPackageLeader\\nOther', 'example': 'DataCollector'}, 'Identifier description': {'type': 'string', 'required': 'Y', 'description': 'A description of the referent of the related identifier.', 'example': 'The protocol use to generate this dataset.'}, 'Relation type': {'type': 'string', 'required': 'Y', 'description': 'The relationship of this dataset to its related identifier. For example, if the identifier is the originating article, it would be “this dataset IsDescribedBy the originating article”. Options are\\nIsProtocolFor\\nHasProtocol\\nIsSoftwareFor\\nHasSoftware\\nIsCitedBy\\nCites\\nIsSupplementTo\\nIsSupplementedBy\\nIsContinuedByContinues\\nIsDescribedBy\\nDescribes\\nHasMetadata\\nIsMetadataFor\\nHasVersion\\nIsVersionOf\\nIsNewVersionOf\\nIsPreviousVersionOf\\nIsPartOf\\nHasPart\\nIsPublishedIn\\nIsReferencedBy\\nReferences\\nIsDocumentedBy\\nDocuments\\nIsCompiledBy\\nCompiles\\nIsVariantFormOf\\nIsOriginalFormOf\\nIsIdenticalTo\\nIsReviewedBy\\nReviews\\nIsDerivedFrom\\nIsSourceOf\\nIsRequiredBy\\nRequires\\nIsObsoletedBy\\nObsoletes', 'example': 'HasProtocol'}, 'Identifier': {'type': 'string', 'required': 'Y', 'description': 'The identifier for something related to this dataset. For example, the DOI or the relative path to the protocol generated by the Protocols.io, and/or the UUID generated by 12L digital twin platform.', 'example': 'https://doi.org/10.13003/5jchdy'}, 'Identifier type': {'type': 'string', 'required': 'Y', 'description': 'The type of the identifier. For example, DOI/path for protocol, and/or an UUID if referencing to a primary dataset already deposited in the 12L digital twin platform.', 'example': 'DOI'}, 'Number of subjects': {'type': 'integer', 'required': 'Y', 'description': 'Number of unique subjects in this dataset, should match subjects metadata file.', 'example': 1}, 'Number of samples': {'type': 'integer', 'required': 'Y', 'description': 'Number of unique samples in this dataset, should match samples metadata file. Set to zero if there are no samples. ', 'example': 0}}\n",
      "{'type': 'string', 'required': 'Y', 'description': 'Brief description of the study and the data set. Equivalent to the abstract of a scientific paper. Include the rationale for the approach, the types of data collected, the techniques used, formats and number of files and an approximate size.', 'example': 'A really cool dataset that I collected to answer some question.'}\n"
     ]
    }
   ],
   "source": [
    "des_schema = schema.get_schema(\"dataset_description\")\n",
    "print(des_schema.get('Subtitle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a `dataset_description` object for the metadata file by using the `get_metadata` function\n",
    "\n",
    "    Again, dataset_description metadata is used as an exmaple here and feel free to replace the category with the name of the metadata file you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_description = dataset.get_metadata(category=\"dataset_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the metadata's associated values\n",
    "\n",
    "    The `get_values(field_name: str)` method allows to retrieve values from a specific row or column of the metadata file by providing the row name or column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxu759\\Documents\\digital-twin-platform-workshop\\digitaltwins-api\\venv\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1641: DeprecationWarning:np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value      PrincipalInvestigator\n",
       "Value 2      CorrespondingAuthor\n",
       "Value 3                      NaN\n",
       "Value n                      NaN\n",
       "Name: 20, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_description.get_values(field_name='Contributor role')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clear all default metadata values before you edit them if you created your datasets from the template\n",
    "    * Clear all metadata values in the dataset_description file\n",
    "    * (Optional) clear the entire row (e.g., `field_name='Contributor role'`) of metadata values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxu759\\Documents\\digital-twin-platform-workshop\\digitaltwins-api\\venv\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1641: DeprecationWarning:np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "c:\\Users\\jxu759\\Documents\\digital-twin-platform-workshop\\digitaltwins-api\\venv\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1641: DeprecationWarning:np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n"
     ]
    }
   ],
   "source": [
    "dataset_description.clear_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the values again. They have now been deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value    <NA>\n",
       "Name: 20, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_description.get_values(field_name='Contributor role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_description.clear_values(field_name='Contributor role')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add/update metadata values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This function allows you to add or update metadata values\n",
    "\n",
    "    `add_values(*values: Any, field_name: str = '', append: bool = True)`\n",
    "\n",
    "    * `*values` allows single or multiple string values for metadata values.\n",
    "    * [TO DO] `field_name` represents either the row name or the column name, or both. Row name in the `dataset_description` and `code_description` metadata files or elements in other metadata files. It can also take the column heading in metadata file. (The default value of header in `dataset_description` and `code_description` is `Value`. Feel free to specify your own header value.)\n",
    "    * `append` takes a boolean value. The default value is `True`, which appends an element to the end of the list. If the `append` is set to `False`, the values will be overwritten/replaced with the new values you specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding values by rows and columns\n",
    "    * By rows:\n",
    "        * Only `dataset_description` and `code_description` metadata files have both row and coloum headings while other files only have the column heading. Therefore, it is recommended to use the `row_name` parameter and add values by rows in `dataset_description` and `code_description` metadata files.\n",
    "             ```python\n",
    "            dataset_description.add_values(*[\"test1\", \"test2\", \"test3\"], row_name=\"contributor role\")\n",
    "\n",
    "            # Also supports column name. The values will begin populating from the cell identified by its row and column names (the default value is \"Value\")\n",
    "            dataset_description.add_values(\"test1\", \"test2\", \"test3\", row_name=\"contributor role\", col_name=\"Value\")\n",
    "            ```\n",
    "        * For adding values in other metadata files, you could insert values by rows without specifying the row_name and col_name name. Notice that the values length must match thee columns length.\n",
    "\n",
    "            ```python\n",
    "            code_parameters.add_values(*[\"breast_append\", \"test1_append\", \"test2_append\", \"test3_append\", \"test4_append\", \"test5..._append\",\"test3_append\", \"test4_append\", \"test5_append\"])\n",
    "            ```\n",
    "    * By columns:\n",
    "        * In metadata files such as code_parameters, manifest, performances, resources, samples, subjects, and submission metadata, it is recommended to use the `col_name` parameter and add values by column.\n",
    "\n",
    "            ```python\n",
    "            code_parameters.add_values(*[\"test1_name\", \"test2_name\", \"test3_name\", \"test4_name\"], col_name='name')\n",
    "            ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_description.add_values(\"2.0.0\", row_name='metadataversion')\n",
    "dataset_description.add_values(\"experimental\", row_name='type')\n",
    "dataset_description.add_values(\"Duke breast cancer MRI preprocessing\", row_name='Title')\n",
    "dataset_description.add_values(\"\"\"Preprocessing the breast cancer MRI images and saving in Nifti format\"\"\",\n",
    "                                      row_name='subtitle')\n",
    "dataset_description.add_values(\"Breast cancer\", \"image processing\", row_name='Keywords')\n",
    "dataset_description.add_values(\"\"\"Preprocessing the breast cancer MRI images and saving in Nifti format\"\"\",\n",
    "                                      row_name=\"Study purpose\")\n",
    "dataset_description.add_values(\"derived from Duke Breast Cancer MRI dataset\",\n",
    "                                      row_name='Study data Collection')\n",
    "dataset_description.add_values(\"pri\", row_name='Study primary conclusion')\n",
    "dataset_description.add_values(\"mary\", row_name='Study primary conclusion')\n",
    "dataset_description.add_values(\"breast\", row_name='Study organ system')\n",
    "dataset_description.add_values(\"image processing\", row_name='Study approach')\n",
    "dataset_description.add_values(\"\"\"dicom2nifti\"\"\", row_name='Study technique')\n",
    "dataset_description.add_values(\"Lin, Chinchien\", \"Gao, Linkun\", row_name='contributorname')\n",
    "dataset_description.add_values(\"Prasad\", \"Jiali\", row_name='contributorNAME')\n",
    "dataset_description.add_values(*[\"bob\", \"db\"], row_name=\"contributor name\")\n",
    "dataset_description.add_values(\n",
    "    \"https://orcid.org/0000-0001-8170-199X\",\n",
    "    \"https://orcid.org/0000-0001-8171-199X\",\n",
    "    \"https://orcid.org/0000-0001-8172-199X\",\n",
    "    \"https://orcid.org/0000-0001-8173-199X\",\n",
    "    \"https://orcid.org/0000-0001-8174-199X\",\n",
    "    \"https://orcid.org/0000-0001-8176-199X\",\n",
    "    row_name='Contributor orcid')\n",
    "\n",
    "dataset_description.add_values(*[\"University of Auckland\"] * 6, row_name='Contributor affiliation')\n",
    "dataset_description.add_values(*[\"developer\", \"developer\", \"Researcher\", \"Researcher\", \"tester\", \"tester\"],\n",
    "                                      row_name=\"contributor role\")\n",
    "dataset_description.add_values(\"source\", row_name='Identifier description')\n",
    "dataset_description.add_values(\"WasDerivedFrom\", row_name='Relation type')\n",
    "dataset_description.add_values(\"DTP-UUID\", row_name='Identifier')\n",
    "dataset_description.add_values(\"12L digital twin UUID\", row_name='Identifier type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Display the values you just added\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value       developer\n",
      "Value 1     developer\n",
      "Value 2    Researcher\n",
      "Value 3    Researcher\n",
      "Value 4        tester\n",
      "Value 5        tester\n",
      "Name: 20, dtype: object\n"
     ]
    }
   ],
   "source": [
    "values = dataset_description.get_values(field_name=\"contributorrole\")\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove values by row or column\n",
    "\n",
    "    The `remove_values(*values: Any, field_name: str)` method can be used to remove values from a specific row or column of the metadata file. Two arguments are required for this method: values you want to remove and the row name or column name of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value       developer\n",
      "Value 1     developer\n",
      "Value 2    Researcher\n",
      "Value 3    Researcher\n",
      "Value 4          <NA>\n",
      "Value 5          <NA>\n",
      "Name: 20, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset_description.remove_values(\"tester\", field_name=\"contributor role\")\n",
    "\n",
    "values = dataset_description.get_values(field_name=\"contributorrole\")\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add your actual data to dataset 'primary' folders (derivative data will be covered in the next workshop)\n",
    "\n",
    "In the research drive, you'll find a folder named 'test_data' which contains the test datasets for this section of the tutorial. Please move the 'test_data' folder into the tutorial directory.\n",
    "* To comply with SDS framwork, the naming of subjects and samples folders MUST be in this format:  sub-xx (for subjects), sam-xx (for samples).\n",
    "\n",
    "* Copy the data, which can be either folders or files, from the raw sample data folder to the SDS dataset folder that you specified at the beginning of the tutorial.\n",
    "    * Add subject(s) - add subject(s) folder along with its subject and sample metadata.\n",
    "    * Add sample(s) -  add a single sample file or multiple sample files to the dataset sample folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After updating subject or sample data, the following metadata files will be updated __automatically__: dataset_description.xlsx, manifest.txt, samples.xlsx, subjects.xslx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a subject folder that is named with the subject ID. Copy data from primary source data to SDS dataset directory.\n",
    "dataset.add_subject(source_path=\"test_data/bids_data/sub-01\", subject=\"sub-1\", subject_metadata={\n",
    "    \"subject experimental group\": \"experimental\",\n",
    "    \"age\": \"041Y\",\n",
    "    \"sex\": \"F\",\n",
    "    \"species\": \"human\",\n",
    "    \"strain\": \"tissue\",\n",
    "    \"age category\": \"middle adulthood\"\n",
    "}, sample_metadata={\n",
    "    \"sample id\": \"\",\n",
    "    \"subject id\": \"\",\n",
    "    \"sample experimental group\": \"experimental\",\n",
    "    \"sample type\": \"tissue\",\n",
    "    \"sample anatomical location\": \"breast tissue\",\n",
    "})\n",
    "\n",
    "# Add multiple subjects\n",
    "dataset.add_subjects(source_paths=[\"test_data/bids_data/sub-01\",\"./test_data/bids_data/sub-02\"], subjects=[\"sub-1\",\"sub-2\"],subject_metadata={\n",
    "    \"subject experimental group\": \"experimental\",\n",
    "    \"species\": \"human\",\n",
    "    \"strain\": \"tissue\",\n",
    "}, sample_metadata={\n",
    "    \"sample experimental group\": \"experimental\",\n",
    "    \"sample type\": \"tissue\",\n",
    "    \"sample anatomical location\": \"breast tissue\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a primary sample dataset to the template dataset\n",
    "dataset.add_sample(source_path=\"test_data/sample1/raw\", subject=\"sub-xyz\", sample=\"sam-1\",\n",
    "                     data_type=\"primary\", sds_parent_dir=save_dir)\n",
    "\n",
    "# Add multiple primary sample datasets to the template dataset\n",
    "dataset.add_samples(source_paths=[\"test_data/sample1/raw\",\"./test_data/sample3/raw\"], subject=\"sub-xyz\", samples=[\"sam-1\",\"sam-3\"],\n",
    "                     data_type=\"primary\", sds_parent_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metadata = dataset.get_metadata(\"samples\")\n",
    "subject_metadata = dataset.get_metadata(\"subjects\")\n",
    "\n",
    "def add_values_for_sample_metadata(sample_metadata):\n",
    "    sample_metadata.add_values(*[\"test\"] * 6, col_name=\"was derived from\", append=False)\n",
    "    sample_metadata.add_values(*[\"pool id 1\", \"pool id 2\", \"pool id 3\", \"pool id 4\", \"pool id 5\", \"pool id 6\"],\n",
    "                               col_name=\"pool id\", append=False)\n",
    "    sample_metadata.add_values(*[\"Yes\"] * 5, \"No\", col_name=\"also in dataset\", append=False)\n",
    "    sample_metadata.add_values(*[\"Global\"] * 6, col_name=\"member of\", append=False)\n",
    "    sample_metadata.add_values(\n",
    "        *[\"laboratory 1\", \"laboratory 2\", \"laboratory 3\", \"laboratory 4\", \"laboratory 5\", \"laboratory 6\"],\n",
    "        col_name=\"laboratory internal id\", append=False)\n",
    "    sample_metadata.add_values(*[\"1991-05-25\"] * 3, *[\"1991-06-10\"] * 3, col_name=\"date of derivation\", append=False)\n",
    "\n",
    "    sample_metadata.save()\n",
    "\n",
    "def add_values_for_subject_metadata(subject_metadata):\n",
    "    subject_metadata.add_values(\"test-1\",\"test-2\",\"test-xyz\", col_name='subject experimental group', append=False)\n",
    "    subject_metadata.add_values(\"30y\",\"31y\",\"33y\", col_name='age', append=False)\n",
    "    subject_metadata.add_values(\"M\",\"M\",\"M\", col_name='sex', append=False)\n",
    "    subject_metadata.add_values(\"P\",\"human\",\"P\", col_name='species', append=False)\n",
    "    subject_metadata.add_values(\"test\",\"tissue\",\"test\", col_name='strain', append=False)\n",
    "    subject_metadata.add_values(\"old\",\"old\",\"old\", col_name=\"age category\", append=False)\n",
    "    subject_metadata.add_values(*[\"pool id 1\", \"pool id 2\", \"pool id 3\"],\n",
    "                               col_name=\"pool id\", append=False)\n",
    "    subject_metadata.add_values(*[\"Yes\"] * 3, col_name=\"also in dataset\", append=False)\n",
    "    subject_metadata.add_values(*[\"515dsd1515\",\"da515daa69\", \"515dsa62a\"], col_name=\"RRID for strain\", append=False)\n",
    "    subject_metadata.add_values(*[\"Global\"] * 3, col_name=\"member of\", append=False)\n",
    "    subject_metadata.add_values(\n",
    "        *[\"laboratory 1\", \"laboratory 2\", \"laboratory 3\"],\n",
    "        col_name=\"laboratory internal id\", append=False)\n",
    "    subject_metadata.add_values(*[\"1996-03-25\",\"1995-09-05\", \"1996-04-11\"], col_name=\"date of birth\", append=False)\n",
    "    subject_metadata.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_values_for_sample_metadata(sample_metadata)\n",
    "add_values_for_subject_metadata(subject_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_thumbnail(\"./test_data/thumbnail_0.jpg\")\n",
    "dataset.add_thumbnail(\"./test_data/thumbnail_1.jpg\")\n",
    "dataset.delete_data(\"./SDStemplate/primary/thumbnail_0.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save the updated datasets. If you encounter any errors when saving the datasets, try the following:\n",
    "        \n",
    "    * Clear outputs and restart the kernel. Run the code again.\n",
    "    * Run this code in the Jupyter Notebook in browser directly instead of running in IDEs such as Pycharm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save(save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validate the dataset descriptions within the metadata files you just updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target instance: {'Metadata Version': '2.0.0', 'Type': 'experimental', 'Title': 'Duke breast cancer MRI preprocessing', 'Subtitle': 'Preprocessing the breast cancer MRI images and saving in Nifti format', 'Keywords': 'Breast cancer', 'Study purpose': 'Preprocessing the breast cancer MRI images and saving in Nifti format', 'Study data collection': 'derived from Duke Breast Cancer MRI dataset', 'Study primary conclusion': 'pri', 'Study organ system': 'breast', 'Study approach': 'image processing', 'Study technique': 'dicom2nifti', 'Contributor name': 'Lin, Chinchien', 'Contributor ORCiD': 'https://orcid.org/0000-0001-8170-199X', 'Contributor affiliation': 'University of Auckland', 'Contributor role': 'developer', 'Identifier description': 'source', 'Relation type': 'WasDerivedFrom', 'Identifier': 'DTP-UUID', 'Identifier type': '12L digital twin UUID', 'Number of subjects': 3, 'Number of samples': 6}\n",
      "Validation: Passed\n",
      "Target instance: {'subject id': 'sub-1', 'pool id': 'pool id 1', 'subject experimental group': 'test-1', 'age': '30y', 'sex': 'M', 'species': 'P', 'strain': 'test', 'RRID for strain': '515dsd1515', 'age category': 'old', 'also in dataset': 'Yes', 'member of': 'Global', 'laboratory internal id': 'laboratory 1', 'date of birth': '1996-03-25'}\n",
      "Validation: Passed\n",
      "Target instance: {'subject id': 'sub-2', 'pool id': 'pool id 2', 'subject experimental group': 'test-2', 'age': '31y', 'sex': 'M', 'species': 'human', 'strain': 'tissue', 'RRID for strain': 'da515daa69', 'age category': 'old', 'also in dataset': 'Yes', 'member of': 'Global', 'laboratory internal id': 'laboratory 2', 'date of birth': '1995-09-05'}\n",
      "Validation: Passed\n",
      "Target instance: {'subject id': 'sub-xyz', 'pool id': 'pool id 3', 'subject experimental group': 'test-xyz', 'age': '33y', 'sex': 'M', 'species': 'P', 'strain': 'test', 'RRID for strain': '515dsa62a', 'age category': 'old', 'also in dataset': 'Yes', 'member of': 'Global', 'laboratory internal id': 'laboratory 3', 'date of birth': '1996-04-11'}\n",
      "Validation: Passed\n"
     ]
    }
   ],
   "source": [
    "description_meta = schema.load_data(\"./SDStemplate/dataset_description.xlsx\")\n",
    "validator.validate(description_meta, category=\"dataset_description\", version=\"2.0.0\")\n",
    "sub_meta = schema.load_data(\"./SDStemplate/subjects.xlsx\")\n",
    "validator.validate(sub_meta, category=\"subjects\", version=\"2.0.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
